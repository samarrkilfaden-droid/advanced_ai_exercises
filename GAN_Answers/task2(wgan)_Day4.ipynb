{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzVCHdoRLeMGtSgRFnNTxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samarrkilfaden-droid/advanced_ai_exercises/blob/main/task2(wgan)_Day4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_BPp8w6QcX5f"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Data  (CIFAR-10 32×32)\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])  # OK for Tanh normalizes the ranges from -1 to 1\n",
        "])\n",
        "ds = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
        "loader = DataLoader(ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVfO6Hdjcc3H",
        "outputId": "fcc61824-df6a-40b3-a779-20000f90e991"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Hyperparams\n",
        "# ---------------------------\n",
        "z_dim = 128\n",
        "g_lr  = 2e-4\n",
        "d_lr  = 2e-4\n",
        "n_critic = 5                     # critic should be 5 not 1 in wgans\n",
        "lambda_gp = 10.0"
      ],
      "metadata": {
        "id": "UIBX8Vtec6CB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, ch=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, ch,   4, 2, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ch, ch*2, 4, 2, 1),\n",
        "            nn.InstanceNorm2d(ch*2, affine=True),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ch*2, ch*4, 4, 2, 1),\n",
        "            nn.InstanceNorm2d(ch*4, affine=True),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ch*4, 1, 4, 1, 0),\n",
        "            #nn.Sigmoid()                 # remove sigmoid\n",
        "        )\n",
        "    def forward(self, x):\n",
        "       return self.net(x).view(x.size(0))\n",
        "\n",
        "class Gen(nn.Module):\n",
        "    def __init__(self, z=128, ch=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z,   ch*4, 4, 1, 0, bias=False), nn.BatchNorm2d(ch*4), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ch*4, ch*2, 4, 2, 1, bias=False), nn.BatchNorm2d(ch*2), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ch*2, ch,   4, 2, 1, bias=False), nn.BatchNorm2d(ch),   nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ch,   3,    4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z):\n",
        "      return self.net(z.view(z.size(0), z.size(1), 1, 1))\n",
        "\n",
        "D = Critic().to(device)\n",
        "G = Gen(z_dim).to(device)"
      ],
      "metadata": {
        "id": "Am9nTxlHc8sh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Loss placeholders & Optimizers (WRONG for WGAN… intentionally)\n",
        "# ---------------------------\n",
        "# Loss (no BCE, just mean differences)\n",
        "def d_loss(real_scores, fake_scores):\n",
        "    return torch.mean(fake_scores) - torch.mean(real_scores)\n",
        "\n",
        "def g_loss(fake_scores):\n",
        "    return -torch.mean(fake_scores)\n",
        "\n",
        "# Optimizers\n",
        "optG = torch.optim.RMSprop(G.parameters(), lr=5e-5) # recommended for WGAN\n",
        "optD = torch.optim.RMSprop(D.parameters(), lr=5e-5) # recommended for WGAN"
      ],
      "metadata": {
        "id": "FrxNgtHgc_2-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Broken gradient penalty\n",
        "# ---------------------------\n",
        "def gradient_penalty(Dnet, real, fake):\n",
        "    b = real.size(0)\n",
        "    eps = torch.randn(b,1,1,1 ,device=real.device)             # BUG\n",
        "    x_hat = eps*real + (1-eps)*fake\n",
        "    # BUG\n",
        "    d_hat = Dnet(x_hat)\n",
        "    grads = torch.autograd.grad(d_hat.sum(), x_hat, retain_graph=True)[0]    # WRONG\n",
        "    gp = lambda_gp * (grads.view(b, -1).norm(dim=1) - 1.0).mean()            # WRONG\n",
        "    return gp"
      ],
      "metadata": {
        "id": "ieWewopBdCT8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Training loop (intentionally wrong)\n",
        "# ---------------------------\n",
        "for step, (real, _) in enumerate(loader):\n",
        "    real = real.to(device)\n",
        "    b = real.size(0)\n",
        "\n",
        "    # -- Critic updates --\n",
        "    for _ in range(n_critic):                    # BUG\n",
        "        z = torch.randn(b, z_dim, device=device)\n",
        "        fake = G(z).detach()\n",
        "\n",
        "        # WRONG\n",
        "        d_real = D(real)\n",
        "        d_fake = D(fake)\n",
        "\n",
        "        lossD = torch.mean(fake) - torch.mean(real) + \\\n",
        "                gradient_penalty(D, real, fake)\n",
        "\n",
        "        lossD.backward()\n",
        "        optD.step()\n",
        "        optD.zero_grad()\n",
        "\n",
        "\n",
        "        # BUG\n",
        "        #for p in D.parameters():\n",
        "            #p.data.clamp_(-0.01, 0.01)\n",
        "\n",
        "    # -- Generator update --\n",
        "    z = torch.randn(b, z_dim, device=device)\n",
        "    fake = G(z)\n",
        "    # WRONG\n",
        "    lossG = -torch.mean(fake)   # BUG\n",
        "    lossG.backward()\n",
        "    optG.step()\n",
        "    optG.zero_grad()                        # BUG\n",
        "\n",
        "    if step > 10:   # keep the broken demo short\n",
        "        break\n",
        "\n",
        "print(\"Your task: fix all bugs until the WGAN-GP training runs stably.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "IZhLFeOXdFH6",
        "outputId": "3423f67a-f3a4-449f-ab9c-9d96b910d34c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "One of the differentiated Tensors does not require grad",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1647136336.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlossD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlossD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3042779090.py\u001b[0m in \u001b[0;36mgradient_penalty\u001b[0;34m(Dnet, real, fake)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# BUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0md_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# WRONG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_gp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# WRONG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    501\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Setup\n",
        "# ---------------------------\n",
        "D.train()\n",
        "G.train()\n",
        "\n",
        "lambda_gp = 10.0         # gradient penalty coefficient (standard)\n",
        "n_critic  = 5            # critic steps per generator step (standard)\n",
        "g_lr = d_lr = 5e-5\n",
        "\n",
        "optG = torch.optim.RMSprop(G.parameters(), lr=g_lr)  # WGAN (vanilla) uses RMSprop; GP works fine with it too\n",
        "optD = torch.optim.RMSprop(D.parameters(), lr=d_lr)\n",
        "\n",
        "# If your G is ConvTranspose2d-based (DCGAN-style), z should be [B, z_dim, 1, 1]\n",
        "def sample_z(batch, z_dim, device):\n",
        "    return torch.randn(batch, z_dim, 1, 1, device=device)  # <-- change to (batch, z_dim) if your G expects flat input\n",
        "\n",
        "# ---------------------------\n",
        "# Losses (WGAN)\n",
        "# ---------------------------\n",
        "def d_loss(real_scores, fake_scores):\n",
        "    return fake_scores.mean() - real_scores.mean()\n",
        "\n",
        "def g_loss(fake_scores):\n",
        "    return -fake_scores.mean()\n",
        "\n",
        "# ---------------------------\n",
        "# Gradient Penalty (WGAN-GP)\n",
        "# ---------------------------\n",
        "def gradient_penalty(Dnet, real, fake, lambda_gp=10.0):\n",
        "    b = real.size(0)\n",
        "    # ε ~ U[0,1], broadcast across non-batch dims\n",
        "    eps = torch.rand(b, 1, 1, 1, device=real.device, dtype=real.dtype)\n",
        "    x_hat = eps * real + (1.0 - eps) * fake\n",
        "    x_hat.requires_grad_(True)\n",
        "\n",
        "    d_hat = Dnet(x_hat)                     # [B] or [B,1,...]\n",
        "    d_hat = d_hat.view(b)                   # flatten to [B] if needed\n",
        "\n",
        "    grads = torch.autograd.grad(\n",
        "        outputs=d_hat.sum(),                # scalar\n",
        "        inputs=x_hat,\n",
        "        create_graph=True,                  # retain graph for higher-order grad\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]                                    # [B, C, H, W]\n",
        "\n",
        "    grad_norm = grads.view(b, -1).norm(2, dim=1)     # 2\n",
        "    gp = lambda_gp * ((grad_norm - 1.0) ** 2).mean() # ( - 1)^2\n",
        "    return gp\n",
        "\n",
        "# ---------------------------\n",
        "# Training loop (fixed)\n",
        "# ---------------------------\n",
        "for step, (real, _) in enumerate(loader):\n",
        "    real = real.to(device)\n",
        "    b = real.size(0)\n",
        "\n",
        "    # ---- Critic updates ----\n",
        "    for _ in range(n_critic):\n",
        "        # (1) Sample noise and make fake (detach so D update doesn't backprop to G)\n",
        "        z = sample_z(b, z_dim, device)\n",
        "        with torch.no_grad():\n",
        "            fake = G(z)\n",
        "\n",
        "        # (2) Critic scores\n",
        "        real_scores = D(real).view(b)\n",
        "        fake_scores = D(fake).view(b)\n",
        "\n",
        "        # (3) WGAN loss + GP\n",
        "        gp = gradient_penalty(D, real, fake, lambda_gp=lambda_gp)\n",
        "        lossD = d_loss(real_scores, fake_scores) + gp\n",
        "\n",
        "        # (4) Optimize critic\n",
        "        optD.zero_grad(set_to_none=True)\n",
        "        lossD.backward()\n",
        "        optD.step()\n",
        "\n",
        "    # ---- Generator update ----\n",
        "    z = sample_z(b, z_dim, device)\n",
        "    fake = G(z)\n",
        "    fake_scores = D(fake).view(b)\n",
        "    lossG = g_loss(fake_scores)\n",
        "\n",
        "    optG.zero_grad(set_to_none=True)\n",
        "    lossG.backward()\n",
        "    optG.step()\n",
        "\n",
        "    # (Optional) short-run break for debugging\n",
        "    # if step > 10: break"
      ],
      "metadata": {
        "id": "msQr0KoVmPAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "awI1TvrHmpak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
